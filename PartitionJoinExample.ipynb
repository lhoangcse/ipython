{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# github.com/minrk/findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"Spark1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a large RDD and a small RDD, both indexed by some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = sc.parallelize([(id, \"user {}\".format(id)) \n",
    "                        for id in xrange(1000000)], 20).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events = sc.parallelize([(id, \"did something\")\n",
    "                         for id in xrange(1, 1000000, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining without co-partitioning causes a large shuffle\n",
    "Check http://localhost:4040/stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[(1, ('user 1', 'did something')), (524301, ('user 524301', 'did something')), (393401, ('user 393401', 'did something')), (939401, ('user 939401', 'did something')), (262501, ('user 262501', 'did something'))]\n"
     ]
    }
   ],
   "source": [
    "users_join_events = users.join(events).cache()\n",
    "print users_join_events.count()  # Force a full computation\n",
    "print users_join_events.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-partitioning removes shuffle\n",
    "Note, however, that co-partitioned is not necessarily co-located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users20 = users.partitionBy(20).cache()\n",
    "users20.foreach(lambda x: x)  # force users20 to recompute\n",
    "events20 = events.partitionBy(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check http://localhost:4040/stages again.\n",
    "\n",
    "Shuffle cost for these counts should be much lower (~70 KB vs. ~10 MB).\n",
    "\n",
    "Note that you should see one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined20 = users20.join(events20)\n",
    "joined20.count()  \n",
    "joined20.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion from class: rdd.take() doesn't cause a full recomputation.\n",
    "\n",
    "I expected this to have 20 jobs (corresponding to the 20 partitions).\n",
    "\n",
    "Note that events20 is not cached, so it should be recomputed, by default.  But we only see 4 jobs at http://localhost:4040/stages \n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('user 1', 'did something')),\n",
       " (524301, ('user 524301', 'did something')),\n",
       " (240301, ('user 240301', 'did something')),\n",
       " (131101, ('user 131101', 'did something')),\n",
       " (262201, ('user 262201', 'did something')),\n",
       " (393301, ('user 393301', 'did something')),\n",
       " (101, ('user 101', 'did something')),\n",
       " (524401, ('user 524401', 'did something')),\n",
       " (87401, ('user 87401', 'did something')),\n",
       " (131201, ('user 131201', 'did something'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined20.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out you have to look at the Spark source to figure this out.\n",
    "\n",
    "https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L1298\n",
    "\n",
    "The answer is: rdd.take() first tries to get enough data from one partition, and if there isn't enough, tries increasingly large numbers until it gets enough.\n",
    "\n",
    "But this adds more confusion.  Why isn't there enough data in the first partition?\n",
    "\n",
    "Let's see how data is distributed between partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined20.mapPartitions(lambda seq: [len(list(seq))]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suboptimal interaction between hash values and partition size.\n",
    "\n",
    "Events run from 1 to 1,000,000 in steps of 100.  \n",
    "Hashes of integer keys are (usually) just that integer.  \n",
    "The destination partition for key K is hash(K) % num_paritions.  \n",
    "\n",
    "--> Our problem is that this will always be 1 for our event keys.\n",
    "\n",
    "### Solution: use a prime number for partition size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users23 = users.partitionBy(23).cache()\n",
    "users23.foreach(lambda x: x)  # force users2 to recompute\n",
    "events23 = events.partitionBy(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined23 = users23.join(events23)\n",
    "joined23.count()  \n",
    "joined23.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how balanced the partitions are, now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[434,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 434,\n",
       " 434,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 434,\n",
       " 434,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435,\n",
       " 435]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined23.mapPartitions(lambda seq: [len(list(seq))]).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
